import pandas as pd
import numpy as np
from gluonts.dataset.pandas import PandasDataset
from gluonts.torch.model.deepar import DeepAREstimator
import gc
from statsforecast import StatsForecast
from statsforecast.models import Naive, SeasonalNaive, WindowAverage, AutoARIMA, AutoETS, AutoCES, AutoTheta, CrostonOptimized, CrostonSBA, ADIDA, IMAPA, SeasonalWindowAverage
from statsforecast.utils import ConformalIntervals

###CONTROLS###
#The below functions allow you to control some of the variables in the script

split_percentage = 90 #This number controls what percentage of our sales data we train our models on, with the remaining amount used to test

gc.collect() # This basically tells the system to empty unallocated memory slots for efficiency purposes

# Load the dataset (Selects the source Excel file and appropriate workbook)
file_path = 'C:\\PythonLocal\\Forecast\\ForecastData.xlsx'
base_data = pd.read_excel(file_path, sheet_name='Sales')

# Optionally limit the dataset to randomly selected item codes for testing purposes
## UNCOMMENT THE FOLLOWING LINE TO ACTIVATE RANDOM SAMPLES##
#base_data = base_data[base_data['Item Code'].isin(base_data['Item Code'].sample(5).unique())]

# Set Data Types (This sets a few predefined data types, and then sets any non-defined column to 'category' type)
predefined_dtypes = {
    'Item Code': 'str', 
    'Week Commencing': 'datetime64[ns]', 
    'Total Weight (KG)': 'float32'
}

for col in base_data.columns:
    if col in predefined_dtypes:
        base_data[col] = base_data[col].astype(predefined_dtypes[col])
    else:
        base_data[col] = base_data[col].astype('category')

# Create item_master and sales_data (Creates an Item Master dataframe, and isolates just the transactional data in to a Sales dataframe)
item_master = (base_data.drop(columns=['Week Commencing', 'Total Weight (KG)'])
               .drop_duplicates()
               .rename(columns={'Item Code': 'unique_id'}, inplace=False)
               .reset_index(drop=True))

sales_data = (base_data[['Item Code', 'Week Commencing', 'Total Weight (KG)']]
              .pivot(index='Item Code', columns='Week Commencing', values='Total Weight (KG)')
              .fillna(0)
              .stack()
              .reset_index()
              .rename(columns={'Item Code': 'unique_id', 'Week Commencing': 'ds', 0: 'y'}, inplace=False))

# Calculate the split of train and test data (The split percentage can be set in the controls at the top of the script)
split_date = sales_data['ds'].min() + (split_percentage/100) * (sales_data['ds'].max() - sales_data['ds'].min())
train_data = sales_data[sales_data['ds'] < split_date].reset_index(drop=True)
test_data = sales_data[sales_data['ds'] >= split_date].reset_index(drop=True)
h = test_data['ds'].nunique() # When testing our models, this ensures that the length of the data series we test is always equal to the predefined split we calculated

# Categorical Column Set (this identifies the categorical columns in item_master by excluding the item code)
categorical_columns = [col for col in item_master.columns if col != 'unique_id']

# Training Data with Exogenous Factors (this joins the categorical columns to the sales data to allow our models to use the categories)
train_data_exog = pd.merge(train_data, item_master[categorical_columns + ['unique_id']], on='unique_id', how='left')

# Wide format training data (this converts our newly merged training data in to a wide format needed for deep learning models)
train_data_wide = PandasDataset.from_long_dataframe(train_data_exog, 
                                             target='y', 
                                             item_id='unique_id', 
                                             timestamp='ds', 
                                             freq='W-MON', 
                                             static_feature_columns=categorical_columns)

# Cleanup (getting rid of memory hungry dataframes that won't be needed again)
del base_data
gc.collect() # This basically tells the system to empty unallocated memory slots for efficiency purposes

# Defining sMAPE calculatuion
def smape(y_true, y_pred):
    if y_true.ndim > 1:
        y_true = y_true.reshape(-1)
    if y_pred.ndim > 1:
        y_pred = y_pred.reshape(-1)

    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_true - y_pred) / denominator
    diff[denominator == 0] = 0.0
    return (100 * np.mean(diff)) / 2
    
# Defining Coverage Probability % calculation
def coverage_prob(y_true, y_lo, y_hi):
    return np.mean((y_lo <= y_true) & (y_true <= y_hi))

# Defines the deep learning predictor
def deepar_pred(predictor, dataset):
        pred = list(predictor.predict(dataset))
        all_preds = list()
        for item in pred:
            unique_id = item.item_id
            p = item.samples.mean(axis=0)
            plo = np.percentile(item.samples, 5, axis=0)
            phi = np.percentile(item.samples, 95, axis=0)
            dates = pd.date_range(start=item.start_date.to_timestamp(), periods=len(p), freq='W-MON')
            category_pred = pd.DataFrame({'ds': dates, 'unique_id': unique_id, 'pred': p, 'plo': plo, 'phi': phi})
            all_preds += [category_pred]
        all_preds = pd.concat(all_preds, ignore_index=True)
        return all_preds

# Defines the trial of paramaters for the deep learning predictor
def deepar_param_set(trial):
    num_layers = trial.suggest_int('num_layers', 1, 4)
    context_length = trial.suggest_int('context_length', 4, 52)
    hidden_size = trial.suggest_categorical('hidden_size', [8, 16, 32, 64, 128, 256])
    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.95)
    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)
    batch_size = 32
    
    estimator = DeepAREstimator(freq='W-MON', prediction_length=h, 
                            num_feat_static_cat=len(categorical_columns),
                            cardinality=[item_master[col].nunique() for col in categorical_columns],
                            num_layers=num_layers,
                            context_length=context_length, hidden_size=hidden_size,
                            dropout_rate=dropout_rate, batch_size=batch_size, lr=lr,
                            trainer_kwargs={'accelerator': 'cpu','max_epochs':10})
    predictor = estimator.train(train_data_wide)
    
    deepar_preds = deepar_pred(predictor, train_data_wide)
    deepar_preds = deepar_preds.merge(test_data, on=['ds', 'unique_id'], how='left', suffixes=('_pred', '_true'))
    
    return smape(deepar_preds['y_true'], deepar_preds['y_pred'])

def main():
    models = [
        ('Naive', Naive()), 
        ('Seasonal Naive', SeasonalNaive(season_length=52)),
        ('Seasonal Average', SeasonalWindowAverage(season_length=52, window_size=4)),
        ('Window Average', WindowAverage(window_size=4)),
        ('ARIMA', AutoARIMA(season_length=52, stepwise=True)),
        ('Simple Exp Smoothing', AutoETS(season_length=52)),
        ('Complex Exp Smoothing', AutoCES(season_length=52)),
        ('Theta', AutoTheta(season_length=52)),
        ('Croston Optimized', CrostonOptimized()),
        ('Croston SBA', CrostonSBA()),
        ('ADIDA', ADIDA()),
        ('IMAPA', IMAPA())
    ]

        # Create a copy of test_data to store all forecasts
    combined_forecast = test_data.copy()

    for model_name, model in models:
        print(f"Modelling {model_name}")

        # Create and fit the model
        baseline_model = StatsForecast(models=[model], freq='W-MON', n_jobs=-1, fallback_model=Naive())
        baseline_model.fit(train_data)
        print(f"Model {model_name} fitted.")

        print(f"Forecasting {model_name}")

        # Forecast
        forecast = baseline_model.forecast(h=h, level=[90], prediction_intervals=ConformalIntervals(h=h))
        forecast = forecast.reset_index().merge(test_data, on=['ds', 'unique_id'], how='left')

        # Only keep model-specific columns (excluding 'unique_id' and 'ds')
        model_specific_columns = [col for col in forecast.columns if col not in combined_forecast.columns]

        # Merge these columns into the combined_forecast DataFrame
        combined_forecast = combined_forecast.merge(forecast[['unique_id', 'ds'] + model_specific_columns], on=['unique_id', 'ds'], how='left')

        print(f"Forecasting for {model_name} completed.")

    return combined_forecast

if __name__ == '__main__':
    combined_forecast = main()
